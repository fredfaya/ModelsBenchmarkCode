{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import metrics\n",
    "\n",
    "import mlflow\n",
    "from pyngrok import ngrok\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# chargement des données\n",
    "data = pd.read_csv('Outputs/data.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "lib_etendu       0\nbilan            0\nmontant_signe    0\nsigne            0\nnum_oper         0\ndiff_ope_val     0\ndtype: int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# On va maintenant creer un pipeline pour le preprocessing des données et le model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# On va recuperer les colonnes numeriques et categoricielles des donnees\n",
    "\n",
    "# On recupere les colonnes numeriques\n",
    "numerical_features = data.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "# On recupere les colonnes categoricielles\n",
    "categorical_features = data.select_dtypes(include=['object']).columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# On va creer une foncion global qui nous permettra de creer un pipeline avec des caracteristiques differentes\n",
    "\n",
    "def create_pipeline(model = None, transform_for_num = None):\n",
    "\n",
    "    numerical_pipeline = make_pipeline(\n",
    "        SimpleImputer(strategy='most_frequent'),\n",
    "        transform_for_num\n",
    "    )\n",
    "    # On ne fera plus de transformations sur les variables categoricielles\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_pipeline, numerical_features)\n",
    "        ]\n",
    "    )\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ]\n",
    "    )\n",
    "    return pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# On va maintenant creer des experiences avec MLflow afin de determiner le meilleur model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On va etudier differents models :\n",
    "- Clustering-Based Local Outlier Factor\n",
    "- Isolation Forest\n",
    "- One-Class SVM\n",
    "- K-Means\n",
    "- DBSCAN\n",
    "- Autoencoder\n",
    "\n",
    "Nous utiliserons un mode d'apprentissage non supervisee pour detecter les anomalies. Donc pour les metrics d'evaluation, nous allons utiliser :\n",
    "- silhouette_score : mesure la distance entre chaque point de données et les points de données de son cluster voisin le plus proche par rapport à la distance moyenne de tous les points de données dans le cluster. Un score élevé indique une bonne séparation des clusters.\n",
    "- inertia : la quantité de variance des données qui est expliquée par les clusters. Plus l'inertie est faible, plus les clusters sont compacts et homogènes\n",
    "- calinski_harabasz_score :  mesure la séparation entre les clusters. Plus la variance inter-cluster est grande par rapport à la variance intra-cluster, plus le score est élevé, ce qui indique que les clusters sont bien séparés.\n",
    "- davies_bouldin_score : est calculé en mesurant la distance entre chaque paire de clusters et en comparant cette distance à la somme des rayons des deux clusters. Un score plus faible indique que les clusters sont plus compacts et séparés."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On va commencer par le model K-Means"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/20 13:05:37 WARNING mlflow.utils: Truncated the value of the key `steps`. Truncated value: `[('preprocessor', ColumnTransformer(transformers=[('num',\n",
      "                                 Pipeline(steps=[('simpleimputer',\n",
      "                                                  SimpleImputer(strategy='most_frequent')),\n",
      "                                                 ('standardscaler',\n",
      "                                                  StandardScaler())]),\n",
      "                                 Index(['lib_etendu', 'bilan', 'montant_signe', 'signe', 'num_oper',\n",
      "       'diff_ope_val'],\n",
      "      dtype='obj...`\n",
      "2023/04/20 13:05:37 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n"
     ]
    }
   ],
   "source": [
    "# On va definir l'experience du K-Means\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "mlflow.set_experiment(\"K-Means_experiment\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"default_K-Means_with_StandardScaler\") as run:\n",
    "    # On va creer un pipeline avec le model K-Means\n",
    "    pipeline = create_pipeline(model = KMeans(), transform_for_num = StandardScaler())\n",
    "\n",
    "    # On va entrainer le model\n",
    "    pipeline.fit(data)\n",
    "\n",
    "    # On va recuperer les predictions\n",
    "    predictions = pipeline.predict(data)\n",
    "\n",
    "    # On va ajouter les metrics\n",
    "    mlflow.log_metric(\"silhouette_score\", metrics.silhouette_score(data, predictions))\n",
    "    mlflow.log_metric(\"inertia\", pipeline['model'].inertia_)\n",
    "    mlflow.log_metric(\"calinski_harabasz_score\", metrics.calinski_harabasz_score(data, predictions))\n",
    "    mlflow.log_metric(\"davies_bouldin_score\", metrics.davies_bouldin_score(data, predictions))\n",
    "\n",
    "mlflow.end_run()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# On va definir l'experience du K-Means\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=\"default_K-Means_with_MinMaxScaler\") as run:\n",
    "    # On va creer un pipeline avec le model K-Means\n",
    "    pipeline = create_pipeline(model = KMeans(), transform_for_num = MinMaxScaler())\n",
    "\n",
    "    # On va entrainer le model\n",
    "    pipeline.fit(data)\n",
    "\n",
    "    # On va recuperer les predictions\n",
    "    predictions = pipeline.predict(data)\n",
    "\n",
    "    # On va ajouter les metrics\n",
    "    mlflow.log_metric(\"silhouette_score\", metrics.silhouette_score(data, predictions))\n",
    "    mlflow.log_metric(\"inertia\", pipeline['model'].inertia_)\n",
    "    mlflow.log_metric(\"calinski_harabasz_score\", metrics.calinski_harabasz_score(data, predictions))\n",
    "    mlflow.log_metric(\"davies_bouldin_score\", metrics.davies_bouldin_score(data, predictions))\n",
    "\n",
    "mlflow.end_run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"default_K-Means_with_RobustScaler\") as run:\n",
    "    # On va creer un pipeline avec le model K-Means\n",
    "    pipeline = create_pipeline(model = KMeans(), transform_for_num = RobustScaler())\n",
    "\n",
    "    # On va entrainer le model\n",
    "    pipeline.fit(data)\n",
    "\n",
    "    # On va recuperer les predictions\n",
    "    predictions = pipeline.predict(data)\n",
    "\n",
    "    # On va ajouter les metrics\n",
    "    mlflow.log_metric(\"silhouette_score\", metrics.silhouette_score(data, predictions))\n",
    "    mlflow.log_metric(\"inertia\", pipeline['model'].inertia_)\n",
    "    mlflow.log_metric(\"calinski_harabasz_score\", metrics.calinski_harabasz_score(data, predictions))\n",
    "    mlflow.log_metric(\"davies_bouldin_score\", metrics.davies_bouldin_score(data, predictions))\n",
    "\n",
    "mlflow.end_run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"K-Means_model1_with_StandardScaler\") as run:\n",
    "    # On va creer un pipeline avec le model K-Means\n",
    "    pipeline = create_pipeline(model = KMeans(), transform_for_num = StandardScaler())\n",
    "\n",
    "    # On va entrainer le model\n",
    "    pipeline.fit(data)\n",
    "\n",
    "    # On va recuperer les predictions\n",
    "    predictions = pipeline.predict(data)\n",
    "\n",
    "    # On va ajouter les metrics\n",
    "    mlflow.log_metric(\"silhouette_score\", metrics.silhouette_score(data, predictions))\n",
    "    mlflow.log_metric(\"inertia\", pipeline['model'].inertia_)\n",
    "    mlflow.log_metric(\"calinski_harabasz_score\", metrics.calinski_harabasz_score(data, predictions))\n",
    "    mlflow.log_metric(\"davies_bouldin_score\", metrics.davies_bouldin_score(data, predictions))\n",
    "\n",
    "mlflow.end_run()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Voir les resultats sur le tableau de bord MLflow en utilisant nyngrok"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=2023-04-20T13:18:40+0000 lvl=warn msg=\"ngrok config file found at both XDG and legacy locations, using XDG location\" xdg_path=C:\\\\Users\\\\ADMIN\\\\AppData\\\\Local/ngrok/ngrok.yml legacy_path=C:\\\\Users\\\\ADMIN\\\\.ngrok2\\\\ngrok.yml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Tracking UI:  https://83d1-197-253-215-109.ngrok-free.app\n"
     ]
    }
   ],
   "source": [
    "# On va creer un tunnel pour acceder au tableau de bord MLflow\n",
    "ngrok.kill()\n",
    "\n",
    "NGROK_AUTH_TOKEN = \"2OgxmHRxos2U37s8DO5FlG7Pn3D_3ih56ethLcFPMdEhTBxnS\"\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "ngrok_tunnel = ngrok.connect(addr='5000', proto='http', bind_tls=True)\n",
    "print('MLflow Tracking UI: ', ngrok_tunnel.public_url)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mlflow ui"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
